<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>AI Study on SincereCSL's Blog</title><link>https://sincerecsl.github.io/categories/ai-study/</link><description>Recent content in AI Study on SincereCSL's Blog</description><generator>Hugo</generator><language>zh-CN</language><managingEditor>SincereCSL@163.com (SincereCSL)</managingEditor><webMaster>SincereCSL@163.com (SincereCSL)</webMaster><lastBuildDate>Thu, 25 Dec 2025 10:00:00 +0800</lastBuildDate><atom:link href="https://sincerecsl.github.io/categories/ai-study/index.xml" rel="self" type="application/rss+xml"/><item><title>转载：2025 我最喜欢的 LLM x AI 论文集</title><link>https://sincerecsl.github.io/llm-x-ai/</link><pubDate>Thu, 25 Dec 2025 10:00:00 +0800</pubDate><author>SincereCSL@163.com (SincereCSL)</author><guid>https://sincerecsl.github.io/llm-x-ai/</guid><description>&lt;h2 id="2025-我最喜欢的-llm-x-ai-论文集"&gt;2025 我最喜欢的 LLM x AI 论文集&lt;/h2&gt;
&lt;p&gt;2025年快过去了。&lt;/p&gt;
&lt;p&gt;这一年，我几乎每天都有坚持读论文和分享论文， 获益匪浅。&lt;/p&gt;
&lt;p&gt;下面是我整理的个人年度最喜欢论文集：&lt;/p&gt;
&lt;h3 id="1test-time-scaling"&gt;1、Test Time Scaling&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;“Thinking” 到底是什么？&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id="deepseek-r1-incentivizing-reasoning-capability-in-llms-via-reinforcement-learning"&gt;&lt;a href="https://arxiv.org/pdf/2501.12948" target="_blank" rel="noopener noreffer "&gt;DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning&lt;/a&gt;&lt;/h5&gt;
&lt;h5 id="s1-simple-test-time-scaling"&gt;&lt;a href="https://arxiv.org/pdf/2501.19393" target="_blank" rel="noopener noreffer "&gt;s1: Simple test-time scaling&lt;/a&gt;&lt;/h5&gt;
&lt;h3 id="2-efficient-reasoning"&gt;2. Efficient Reasoning&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;“如果思考一种budget”&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id="l1-controlling-how-long-a-reasoning-model-thinks-with-reinforcement-learning"&gt;&lt;a href="https://arxiv.org/pdf/2503.04697" target="_blank" rel="noopener noreffer "&gt;L1: Controlling How Long A Reasoning Model Thinks With Reinforcement Learning&lt;/a&gt;&lt;/h5&gt;
&lt;h5 id="scalable-chain-of-thoughts-via-elastic-reasoning"&gt;&lt;a href="https://arxiv.org/pdf/2505.05315" target="_blank" rel="noopener noreffer "&gt;Scalable Chain of Thoughts via Elastic Reasoning&lt;/a&gt;&lt;/h5&gt;
&lt;h3 id="3-reasoning-analysis"&gt;3. Reasoning Analysis&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;该如何更好理解 “Reasoning” ？&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id="how-do-reasoning-models-reason"&gt;&lt;a href="https://arxiv.org/pdf/2504.09762" target="_blank" rel="noopener noreffer "&gt;(How) Do reasoning models reason?&lt;/a&gt;&lt;/h5&gt;
&lt;h5 id="deepseek-r1-thoughtology-lets-about-llm-reasoning"&gt;&lt;a href="https://arxiv.org/pdf/2504.07128" target="_blank" rel="noopener noreffer "&gt;DeepSeek-R1 Thoughtology: Let’s about LLM reasoning&lt;/a&gt;&lt;/h5&gt;
&lt;h5 id="rethinking-reflection-in-pre-training"&gt;&lt;a href="https://arxiv.org/pdf/2504.04022" target="_blank" rel="noopener noreffer "&gt;Rethinking Reflection in Pre-Training&lt;/a&gt;&lt;/h5&gt;
&lt;h5 id="thinking-vs-doing-agents-that-reason-by-scaling-test-time-interaction"&gt;&lt;a href="arxiv.org/pdf/2506.07976" rel=""&gt;Thinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction&lt;/a&gt;&lt;/h5&gt;
&lt;h3 id="4-cli-agent"&gt;4. CLI Agent&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;2024年文章，但深度影响了2025。&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id="swe-agent-agent-computer-interfaces-enable-automated-software-engineering"&gt;&lt;a href="https://arxiv.org/pdf/2405.15793" target="_blank" rel="noopener noreffer "&gt;SWE-agent: Agent-Computer Interfaces Enable Automated Software Engineering&lt;/a&gt;&lt;/h5&gt;
&lt;h3 id="5-llm-x-rl-agentic"&gt;5. LLM X RL Agentic&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;为此，我创造了一个词，“协议token”&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id="retool-reinforcement-learning-for-strategic-tool-use-in-llms"&gt;&lt;a href="https://arxiv.org/pdf/2504.11536" target="_blank" rel="noopener noreffer "&gt;ReTool: Reinforcement Learning for Strategic Tool Use in LLMs&lt;/a&gt;&lt;/h5&gt;
&lt;h5 id="research-learning-to-reason-with-search-for-llms-via-reinforcement-learning"&gt;&lt;a href="https://arxiv.org/pdf/2503.19470" target="_blank" rel="noopener noreffer "&gt;ReSearch: Learning to Reason with Search for LLMs via Reinforcement Learning&lt;/a&gt;&lt;/h5&gt;
&lt;h5 id="recall-learning-to-reason-with-tool-call-for-llms-via-reinforcement-learning"&gt;&lt;a href="https://attractive-almandine-935.notion.site/ReCall-Learning-to-Reason-with-Tool-Call-for-LLMs-via-Reinforcement-Learning-1d7aec91e9bb8006ad40f9edbfe2191a" target="_blank" rel="noopener noreffer "&gt;ReCall: Learning to Reason with Tool Call for LLMs via Reinforcement Learning&lt;/a&gt;&lt;/h5&gt;
&lt;h3 id="6-parallel-reasoning"&gt;6. Parallel Reasoning&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;并行 reasoning，超越单线程的想象！&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id="learning-adaptive-parallel-reasoning-with-language-models"&gt;&lt;a href="https://arxiv.org/pdf/2504.15466" target="_blank" rel="noopener noreffer "&gt;Learning Adaptive Parallel Reasoning with Language Models&lt;/a&gt;&lt;/h5&gt;
&lt;h5 id="multiverse-your-language-models-secretly-decide-how-to-parallelize-and-merge-generation"&gt;&lt;a href="https://arxiv.org/pdf/2506.09991" target="_blank" rel="noopener noreffer "&gt;Multiverse: Your Language Models Secretly Decide How to Parallelize and Merge Generation&lt;/a&gt;&lt;/h5&gt;
&lt;h5 id="learning-to-keep-a-promise-scaling-language-model-decoding-parallelism-with-learned-asynchronous-decoding"&gt;&lt;a href="https://arxiv.org/pdf/2502.11517" target="_blank" rel="noopener noreffer "&gt;Learning to Keep a Promise: Scaling Language Model Decoding Parallelism with Learned Asynchronous Decoding&lt;/a&gt;&lt;/h5&gt;
&lt;h3 id="7-rl-x-reasoning"&gt;7. RL X Reasoning,&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;RLVR 这四篇，给 RL X Reasoning 极速升温+降温！&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id="reinforcement-learning-for-reasoning-in-large-language-models-with-one-training-example"&gt;&lt;a href="https://arxiv.org/pdf/2504.20571" target="_blank" rel="noopener noreffer "&gt;Reinforcement Learning for Reasoning in Large Language Models with One Training Example&lt;/a&gt;&lt;/h5&gt;
&lt;h5 id="absolute-zero-reinforced-self-play-reasoning-with-zero-data"&gt;&lt;a href="https://arxiv.org/pdf/2505.03335" target="_blank" rel="noopener noreffer "&gt;Absolute Zero: Reinforced Self-play Reasoning with Zero Data&lt;/a&gt;&lt;/h5&gt;
&lt;h5 id="learning-to-reason-without-external-rewards"&gt;&lt;a href="https://arxiv.org/pdf/2505.19590" target="_blank" rel="noopener noreffer "&gt;Learning to Reason without External Rewards&lt;/a&gt;&lt;/h5&gt;
&lt;h5 id="spurious-rewards-rethinking-training-signals-in-rlvr"&gt;&lt;a href="https://arxiv.org/pdf/2506.10947" target="_blank" rel="noopener noreffer "&gt;Spurious Rewards: Rethinking Training Signals in RLVR&lt;/a&gt;&lt;/h5&gt;
&lt;h3 id="8-agent-interaction-deep-research-agent"&gt;8. Agent, interaction Deep research agent&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;经验，记忆，交互，workflow，围绕&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id="agent-kb-leveraging-cross-domain-experience-for-agentic-problem-solving"&gt;&lt;a href="https://arxiv.org/pdf/2507.06229" target="_blank" rel="noopener noreffer "&gt;Agent KB: Leveraging Cross-Domain Experience for Agentic Problem Solving&lt;/a&gt;&lt;/h5&gt;
&lt;h5 id="alita-g-self-evolving-generative-agent-for-agent-generation"&gt;&lt;a href="https://arxiv.org/pdf/2510.23601" target="_blank" rel="noopener noreffer "&gt;Alita-G: Self-Evolving Generative Agent for Agent Generation&lt;/a&gt;&lt;/h5&gt;
&lt;h5 id="sleep-time-compute-beyond-inference-scaling-at-test-time"&gt;&lt;a href="https://arxiv.org/pdf/2504.13171" target="_blank" rel="noopener noreffer "&gt;Sleep-time Compute: Beyond Inference Scaling at Test-time&lt;/a&gt;&lt;/h5&gt;
&lt;h5 id="webthinker-empowering-large-reasoning-models-with-deep-research-capability"&gt;&lt;a href="https://arxiv.org/pdf/2504.21776" target="_blank" rel="noopener noreffer "&gt;WebThinker: Empowering Large Reasoning Models with Deep Research Capability&lt;/a&gt;&lt;/h5&gt;
&lt;h5 id="webdancer-towards-autonomous-information-seeking-agency"&gt;&lt;a href="https://arxiv.org/pdf/2505.22648" target="_blank" rel="noopener noreffer "&gt;WebDancer: Towards Autonomous Information Seeking Agency&lt;/a&gt;&lt;/h5&gt;
&lt;h3 id="9-risk-modeling-x-llm"&gt;9. Risk modeling X LLM&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;“让贝叶斯再次伟大！”&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id="model-predictive-task-sampling-for-efficient-and-robust-adaptation"&gt;&lt;a href="https://arxiv.org/pdf/2501.11039" target="_blank" rel="noopener noreffer "&gt;Model Predictive Task Sampling for Efficient and Robust Adaptation&lt;/a&gt;&lt;/h5&gt;
&lt;h5 id="can-prompt-difficulty-be-online-predicted-for-accelerating-rl-finetuning-of-reasoning-models"&gt;&lt;a href="https://arxiv.org/pdf/2507.04632" target="_blank" rel="noopener noreffer "&gt;Can Prompt Difficulty be Online Predicted for Accelerating RL Finetuning of Reasoning Models?&lt;/a&gt;&lt;/h5&gt;
&lt;h3 id="10-multi-agentic"&gt;10. Multi-Agentic&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Multi Agent该怎么用？&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id="co-evolving-llm-coder-and-unit-tester-via-reinforcement-learning"&gt;&lt;a href="https://arxiv.org/pdf/2506.03136" target="_blank" rel="noopener noreffer "&gt;Co-Evolving LLM Coder and Unit Tester via Reinforcement Learning&lt;/a&gt;&lt;/h5&gt;
&lt;h5 id="codecontests-high-quality-test-case-generation-for-competitive-programming"&gt;&lt;a href="https://arxiv.org/pdf/2506.05817" target="_blank" rel="noopener noreffer "&gt;CodeContests+: High-Quality Test Case Generation for Competitive Programming&lt;/a&gt;&lt;/h5&gt;
&lt;h3 id="11-sentient-agent"&gt;11. Sentient Agent&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;AI + 人文，情怀满满！&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id="rlver-reinforcement-learning-with-verifiable-emotion-rewards-for-empathetic-agents"&gt;&lt;a href="https://arxiv.org/pdf/2507.03112" target="_blank" rel="noopener noreffer "&gt;RLVER: Reinforcement Learning with Verifiable Emotion Rewards for Empathetic Agents&lt;/a&gt;&lt;/h5&gt;
&lt;h5 id="sentient-agent-as-a-judge-evaluating-higher-order-social-cognition-in-large-language-models"&gt;&lt;a href="https://arxiv.org/pdf/2505.02847" target="_blank" rel="noopener noreffer "&gt;Sentient Agent as a Judge: Evaluating Higher-Order Social Cognition in Large Language Models&lt;/a&gt;&lt;/h5&gt;
&lt;h3 id="12-llm-security-and-alignment"&gt;12: LLM security and alignment&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;想要懂你真的不容易, LLM!&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id="chain-of-thought-monitorability-a-new-and-fragile-opportunity-for-ai-safety"&gt;&lt;a href="https://arxiv.org/pdf/2507.11473" target="_blank" rel="noopener noreffer "&gt;Chain of Thought Monitorability: A New and Fragile Opportunity for AI Safety&lt;/a&gt;&lt;/h5&gt;
&lt;h5 id="emergent-misalignment-narrow-finetuning-can-produce-broadly-misaligned-llms"&gt;&lt;a href="https://arxiv.org/pdf/2502.17424" target="_blank" rel="noopener noreffer "&gt;Emergent Misalignment: Narrow finetuning can produce broadly misaligned LLMs&lt;/a&gt;&lt;/h5&gt;
&lt;h5 id="thought-crime-backdoors-and-emergent-misalignment-in-reasoning-models"&gt;&lt;a href="https://arxiv.org/pdf/2506.13206" target="_blank" rel="noopener noreffer "&gt;Thought Crime: Backdoors and Emergent Misalignment in Reasoning Models&lt;/a&gt;&lt;/h5&gt;
&lt;h5 id="natural-emergent-misalignment-from-reward-hacking-in-production-rl"&gt;&lt;a href="https://arxiv.org/pdf/2511.18397" target="_blank" rel="noopener noreffer "&gt;Natural Emergent Misalignment from Reward Hacking in Production RL&lt;/a&gt;&lt;/h5&gt;
&lt;h5 id="weird-generalization-and-inductive-backdoors-new-ways-to-corrupt-llms"&gt;&lt;a href="https://arxiv.org/pdf/2512.09742" target="_blank" rel="noopener noreffer "&gt;Weird Generalization and Inductive Backdoors: New Ways to Corrupt LLMs&lt;/a&gt;&lt;/h5&gt;
&lt;h3 id="13-model-steering"&gt;13. Model Steering&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;我要控制我自己，LLM!&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id="axbench-steering-llms-even-simple-baselines-outperform-sparse-autoencoders"&gt;&lt;a href="https://arxiv.org/pdf/2501.17148" target="_blank" rel="noopener noreffer "&gt;AxBench: Steering LLMs? Even Simple Baselines Outperform Sparse Autoencoders&lt;/a&gt;&lt;/h5&gt;
&lt;h5 id="persona-vectors-monitoring-and-controlling-character-traits-in-language-models"&gt;&lt;a href="https://arxiv.org/abs/2507.21509" target="_blank" rel="noopener noreffer "&gt;Persona Vectors: Monitoring and Controlling Character Traits in Language Models&lt;/a&gt;&lt;/h5&gt;
&lt;h5 id="how-to-train-your-advisor-steering-black-box-llms-with-advisor-models"&gt;&lt;a href="https://arxiv.org/abs/2510.02453" target="_blank" rel="noopener noreffer "&gt;How to Train Your Advisor: Steering Black-Box LLMs with Advisor Models&lt;/a&gt;&lt;/h5&gt;
&lt;p&gt;最后，这个 list 是我的个人选择，因为时间精力有限，很多很好的工作，或许我都没有机会读到，感谢论文的作者们，带我领略智慧的风光！&lt;/p&gt;</description></item></channel></rss>